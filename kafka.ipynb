{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00714933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Real-time Trump Reddit Sentiment Analysis...\n",
      "Connecting to Kafka stream: reddit-comments-trump\n",
      "Spark session stopped.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 371\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    373\u001b[39m         \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[32m    374\u001b[39m         spark.stop()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 309\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConnecting to Kafka stream: reddit-comments-trump\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Create Kafka stream\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m raw_stream = \u001b[43mcreate_kafka_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Preprocess comments\u001b[39;00m\n\u001b[32m    312\u001b[39m preprocessed_stream = preprocess_reddit_comments(raw_stream)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mcreate_kafka_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_kafka_stream\u001b[39m():\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create Kafka streaming DataFrame\"\"\"\u001b[39;00m\n\u001b[32m     43\u001b[39m     df = \u001b[43mspark\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka.bootstrap.servers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkafka_bootstrap_servers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubscribe\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkafka_topic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstartingOffsets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfailOnDataLoss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxOffsetsPerTrigger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Parse JSON from Kafka value\u001b[39;00m\n\u001b[32m     54\u001b[39m     kafka_df = df.select(\n\u001b[32m     55\u001b[39m         col(\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mkafka_key\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     56\u001b[39m         col(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mkafka_value\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m         col(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mkafka_timestamp\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SarigaMadathil\\OneDrive - ROKO Labs LLC\\Education\\Kafka - sentiment analysis\\.venv\\Lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:307\u001b[39m, in \u001b[36mDataStreamReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(path))\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SarigaMadathil\\OneDrive - ROKO Labs LLC\\Education\\Kafka - sentiment analysis\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SarigaMadathil\\OneDrive - ROKO Labs LLC\\Education\\Kafka - sentiment analysis\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "\n",
    "# Text preprocessing functions\n",
    "def preprocess_reddit_comments(df):\n",
    "    \"\"\"Clean and preprocess Reddit comment text\"\"\"\n",
    "    \n",
    "    # Convert Unix timestamp to readable date\n",
    "    df = df.withColumn(\"created_date\", \n",
    "                      from_unixtime(col(\"created_utc\")).cast(\"timestamp\"))\n",
    "    \n",
    "    # Basic text cleaning for comments\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      regexp_replace(col(\"comment_body\"), r\"http\\S+\", \"\"))  # Remove URLs\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      regexp_replace(col(\"clean_body\"), r\"u/\\w+\", \"\"))  # Remove user mentions\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      regexp_replace(col(\"clean_body\"), r\"r/\\w+\", \"\"))  # Remove subreddit mentions\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      regexp_replace(col(\"clean_body\"), r\"[^a-zA-Z\\s]\", \"\"))  # Keep only letters\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      lower(col(\"clean_body\")))  # Convert to lowercase\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      regexp_replace(col(\"clean_body\"), r\"\\s+\", \" \"))  # Normalize whitespace\n",
    "    df = df.withColumn(\"clean_body\", \n",
    "                      trim(col(\"clean_body\")))  # Remove leading/trailing spaces\n",
    "    \n",
    "    # Filter out empty or very short comments\n",
    "    df = df.filter(col(\"clean_body\").isNotNull() & (length(col(\"clean_body\")) > 5))\n",
    "    \n",
    "    # Add text length for analysis\n",
    "    df = df.withColumn(\"comment_length\", length(col(\"comment_body\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Sentiment analysis using TextBlob\n",
    "def analyze_sentiment_textblob(text):\n",
    "    \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
    "    if text is None or text.strip() == \"\":\n",
    "        return (0.0, 0.0, \"neutral\")\n",
    "    \n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        if polarity > 0.1:\n",
    "            sentiment = \"positive\"\n",
    "        elif polarity < -0.1:\n",
    "            sentiment = \"negative\"\n",
    "        else:\n",
    "            sentiment = \"neutral\"\n",
    "        \n",
    "        return (float(polarity), float(subjectivity), sentiment)\n",
    "    except:\n",
    "        return (0.0, 0.0, \"neutral\")\n",
    "\n",
    "# Register UDF for sentiment analysis\n",
    "sentiment_udf = udf(analyze_sentiment_textblob, \n",
    "                   StructType([\n",
    "                       StructField(\"polarity\", DoubleType(), True),\n",
    "                       StructField(\"subjectivity\", DoubleType(), True),\n",
    "                       StructField(\"sentiment\", StringType(), True)\n",
    "                   ]))\n",
    "\n",
    "def perform_sentiment_analysis(df):\n",
    "    \"\"\"Perform sentiment analysis on Reddit comments\"\"\"\n",
    "    \n",
    "    # Apply sentiment analysis\n",
    "    df = df.withColumn(\"sentiment_analysis\", sentiment_udf(col(\"clean_body\")))\n",
    "    \n",
    "    # Extract sentiment components\n",
    "    df = df.withColumn(\"polarity\", col(\"sentiment_analysis.polarity\")) \\\n",
    "           .withColumn(\"subjectivity\", col(\"sentiment_analysis.subjectivity\")) \\\n",
    "           .withColumn(\"sentiment\", col(\"sentiment_analysis.sentiment\"))\n",
    "    \n",
    "    # Add detailed sentiment categories\n",
    "    df = df.withColumn(\"sentiment_category\",\n",
    "                      when(col(\"polarity\") > 0.5, \"very_positive\")\n",
    "                      .when(col(\"polarity\") > 0.1, \"positive\")\n",
    "                      .when(col(\"polarity\") < -0.5, \"very_negative\")\n",
    "                      .when(col(\"polarity\") < -0.1, \"negative\")\n",
    "                      .otherwise(\"neutral\"))\n",
    "    \n",
    "    # Add engagement indicators\n",
    "    df = df.withColumn(\"engagement_level\",\n",
    "                      when(col(\"score\") > 10, \"high\")\n",
    "                      .when(col(\"score\") > 0, \"medium\")\n",
    "                      .when(col(\"score\") >= -5, \"low\")\n",
    "                      .otherwise(\"very_low\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_real_time_aggregations(df):\n",
    "    \"\"\"Create real-time aggregations with watermarking\"\"\"\n",
    "    \n",
    "    # Add watermark for late data handling\n",
    "    df_watermarked = df.withWatermark(\"kafka_timestamp\", \"10 minutes\")\n",
    "    \n",
    "    # 5-minute windowed sentiment analysis\n",
    "    windowed_sentiment = df_watermarked \\\n",
    "        .groupBy(\n",
    "            window(col(\"kafka_timestamp\"), \"5 minutes\", \"1 minutes\"),\n",
    "            col(\"sentiment\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"comment_count\"),\n",
    "            avg(\"polarity\").alias(\"avg_polarity\"),\n",
    "            avg(\"score\").alias(\"avg_reddit_score\"),\n",
    "            collect_list(\"subreddit\").alias(\"subreddits\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"sentiment\"),\n",
    "            col(\"comment_count\"),\n",
    "            col(\"avg_polarity\"),\n",
    "            col(\"avg_reddit_score\"),\n",
    "            size(array_distinct(col(\"subreddits\"))).alias(\"unique_subreddits\")\n",
    "        )\n",
    "    \n",
    "    # Subreddit-wise sentiment aggregation\n",
    "    subreddit_sentiment = df_watermarked \\\n",
    "        .groupBy(\n",
    "            window(col(\"kafka_timestamp\"), \"10 minutes\", \"5 minutes\"),\n",
    "            col(\"subreddit\"),\n",
    "            col(\"sentiment\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"comment_count\"),\n",
    "            avg(\"polarity\").alias(\"avg_polarity\"),\n",
    "            max(\"score\").alias(\"max_score\"),\n",
    "            min(\"score\").alias(\"min_score\")\n",
    "        )\n",
    "    \n",
    "    return windowed_sentiment, subreddit_sentiment\n",
    "\n",
    "def detect_trending_topics(df):\n",
    "    \"\"\"Detect trending topics and keywords in Trump discussions\"\"\"\n",
    "    \n",
    "    # Extract key phrases (simple approach - can be enhanced with NLP)\n",
    "    df = df.withColumn(\"words\", split(col(\"clean_body\"), \" \"))\n",
    "    df = df.withColumn(\"word_count\", size(col(\"words\")))\n",
    "    \n",
    "    # Explode words for analysis\n",
    "    words_df = df.select(\n",
    "        col(\"comment_id\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"sentiment\"),\n",
    "        col(\"subreddit\"),\n",
    "        explode(col(\"words\")).alias(\"word\")\n",
    "    ).filter(length(col(\"word\")) > 3)  # Filter short words\n",
    "    \n",
    "    # Word frequency with sentiment context\n",
    "    word_sentiment_freq = words_df \\\n",
    "        .withWatermark(\"kafka_timestamp\", \"15 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"kafka_timestamp\"), \"15 minutes\", \"5 minutes\"),\n",
    "            col(\"word\"),\n",
    "            col(\"sentiment\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"word_frequency\"),\n",
    "            countDistinct(\"comment_id\").alias(\"unique_comments\"),\n",
    "            collect_set(\"subreddit\").alias(\"subreddits_mentioned\")\n",
    "        ) \\\n",
    "        .filter(col(\"word_frequency\") >= 3)  # Filter infrequent words\n",
    "    \n",
    "    return word_sentiment_freq\n",
    "\n",
    "# Output sinks for different analyses\n",
    "def setup_output_sinks(processed_df, windowed_sentiment, subreddit_sentiment, word_trends):\n",
    "    \"\"\"Setup various output sinks for streaming results\"\"\"\n",
    "    \n",
    "    # 1. Console output for monitoring\n",
    "    console_query = processed_df.select(\n",
    "        col(\"comment_id\"),\n",
    "        col(\"author\"),\n",
    "        col(\"subreddit\"),\n",
    "        col(\"sentiment\"),\n",
    "        col(\"polarity\"),\n",
    "        col(\"score\"),\n",
    "        substring(col(\"comment_body\"), 1, 100).alias(\"comment_preview\")\n",
    "    ).writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .option(\"numRows\", 10) \\\n",
    "        .trigger(processingTime=\"30 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    # 2. Parquet files for historical analysis\n",
    "    parquet_query = processed_df.select(\n",
    "        col(\"comment_id\"),\n",
    "        col(\"comment_body\"),\n",
    "        col(\"author\"),\n",
    "        col(\"created_date\"),\n",
    "        col(\"score\"),\n",
    "        col(\"subreddit\"),\n",
    "        col(\"sentiment\"),\n",
    "        col(\"polarity\"),\n",
    "        col(\"subjectivity\"),\n",
    "        col(\"sentiment_category\"),\n",
    "        col(\"engagement_level\"),\n",
    "        col(\"kafka_timestamp\")\n",
    "    ).writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", \"/tmp/trump_sentiment_data\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/trump_sentiment_checkpoint_parquet\") \\\n",
    "        .partitionBy(\"sentiment\", \"subreddit\") \\\n",
    "        .trigger(processingTime=\"60 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    # 3. Windowed sentiment aggregations\n",
    "    windowed_console = windowed_sentiment.writeStream \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"60 seconds\") \\\n",
    "        .queryName(\"windowed_sentiment\") \\\n",
    "        .start()\n",
    "    \n",
    "    # 4. Subreddit sentiment trends\n",
    "    subreddit_console = subreddit_sentiment.writeStream \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"120 seconds\") \\\n",
    "        .queryName(\"subreddit_sentiment\") \\\n",
    "        .start()\n",
    "    \n",
    "    # 5. Trending words analysis\n",
    "    trending_words_console = word_trends.writeStream \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"180 seconds\") \\\n",
    "        .queryName(\"trending_words\") \\\n",
    "        .start()\n",
    "    \n",
    "    # 6. Real-time alerts for extreme sentiment\n",
    "    alerts_df = processed_df.filter(\n",
    "        (col(\"polarity\") > 0.7) | (col(\"polarity\") < -0.7)\n",
    "    ).select(\n",
    "        lit(\"SENTIMENT_ALERT\").alias(\"alert_type\"),\n",
    "        col(\"comment_id\"),\n",
    "        col(\"author\"),\n",
    "        col(\"subreddit\"),\n",
    "        col(\"sentiment_category\"),\n",
    "        col(\"polarity\"),\n",
    "        col(\"score\"),\n",
    "        current_timestamp().alias(\"alert_timestamp\"),\n",
    "        substring(col(\"comment_body\"), 1, 200).alias(\"comment_text\")\n",
    "    )\n",
    "    \n",
    "    alerts_query = alerts_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .queryName(\"sentiment_alerts\") \\\n",
    "        .start()\n",
    "    \n",
    "    return {\n",
    "        \"console\": console_query,\n",
    "        \"parquet\": parquet_query,\n",
    "        \"windowed\": windowed_console,\n",
    "        \"subreddit\": subreddit_console,\n",
    "        \"trending\": trending_words_console,\n",
    "        \"alerts\": alerts_query\n",
    "    }\n",
    "\n",
    "# Custom batch processing function\n",
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"Custom processing for each micro-batch\"\"\"\n",
    "    print(f\"\\n=== Processing Batch {batch_id} ===\")\n",
    "    \n",
    "    if batch_df.count() > 0:\n",
    "        # Quick batch statistics\n",
    "        sentiment_counts = batch_df.groupBy(\"sentiment\").count().collect()\n",
    "        avg_polarity = batch_df.agg(avg(\"polarity\")).collect()[0][0]\n",
    "        \n",
    "        print(f\"Batch size: {batch_df.count()}\")\n",
    "        print(f\"Average polarity: {avg_polarity:.3f}\")\n",
    "        print(\"Sentiment distribution:\")\n",
    "        for row in sentiment_counts:\n",
    "            print(f\"  {row['sentiment']}: {row['count']}\")\n",
    "        \n",
    "        # Most extreme sentiments in this batch\n",
    "        if batch_df.filter(col(\"polarity\") > 0.5).count() > 0:\n",
    "            print(\"\\nMost positive comment:\")\n",
    "            batch_df.filter(col(\"polarity\") > 0.5) \\\n",
    "                   .orderBy(desc(\"polarity\")) \\\n",
    "                   .select(\"author\", \"subreddit\", \"polarity\", \"comment_body\") \\\n",
    "                   .limit(1).show(truncate=False)\n",
    "        \n",
    "        if batch_df.filter(col(\"polarity\") < -0.5).count() > 0:\n",
    "            print(\"\\nMost negative comment:\")\n",
    "            batch_df.filter(col(\"polarity\") < -0.5) \\\n",
    "                   .orderBy(\"polarity\") \\\n",
    "                   .select(\"author\", \"subreddit\", \"polarity\", \"comment_body\") \\\n",
    "                   .limit(1).show(truncate=False)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for real-time Trump sentiment analysis\"\"\"\n",
    "    \n",
    "    print(\"Starting Real-time Trump Reddit Sentiment Analysis...\")\n",
    "    print(\"Connecting to Kafka stream: reddit-comments-trump\")\n",
    "    \n",
    "    # Create Kafka stream\n",
    "    raw_stream = create_kafka_stream()\n",
    "    \n",
    "    # Preprocess comments\n",
    "    preprocessed_stream = preprocess_reddit_comments(raw_stream)\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    sentiment_stream = perform_sentiment_analysis(preprocessed_stream)\n",
    "    \n",
    "    # Create aggregations\n",
    "    windowed_sentiment, subreddit_sentiment = create_real_time_aggregations(sentiment_stream)\n",
    "    \n",
    "    # Detect trending topics\n",
    "    word_trends = detect_trending_topics(sentiment_stream)\n",
    "    \n",
    "    # Setup output sinks\n",
    "    queries = setup_output_sinks(sentiment_stream, windowed_sentiment, \n",
    "                               subreddit_sentiment, word_trends)\n",
    "    \n",
    "    # Alternative: Custom batch processing\n",
    "    custom_query = sentiment_stream.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .foreachBatch(process_batch) \\\n",
    "        .trigger(processingTime=\"45 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    print(\"All streaming queries started. Monitoring Trump discussions...\")\n",
    "    print(\"Press Ctrl+C to stop the analysis\")\n",
    "    \n",
    "    try:\n",
    "        # Wait for termination\n",
    "        spark.streams.awaitAnyTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping all streaming queries...\")\n",
    "        for query_name, query in queries.items():\n",
    "            if query.isActive:\n",
    "                query.stop()\n",
    "                print(f\"Stopped {query_name} query\")\n",
    "        \n",
    "        if custom_query.isActive:\n",
    "            custom_query.stop()\n",
    "            print(\"Stopped custom batch processing query\")\n",
    "\n",
    "def monitor_stream_health():\n",
    "    \"\"\"Monitor the health of streaming queries\"\"\"\n",
    "    active_streams = spark.streams.active\n",
    "    \n",
    "    print(f\"\\n=== Stream Health Monitor ===\")\n",
    "    print(f\"Active streams: {len(active_streams)}\")\n",
    "    \n",
    "    for stream in active_streams:\n",
    "        print(f\"\\nStream: {stream.name}\")\n",
    "        print(f\"Status: {stream.status}\")\n",
    "        print(f\"ID: {stream.id}\")\n",
    "        \n",
    "        if len(stream.recentProgress) > 0:\n",
    "            recent = stream.recentProgress[-1]\n",
    "            print(f\"Batch ID: {recent.get('batchId', 'N/A')}\")\n",
    "            print(f\"Input size: {recent.get('inputSize', 'N/A')}\")\n",
    "            print(f\"Processing time: {recent.get('durationMs', {}).get('triggerExecution', 'N/A')} ms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "# Additional utility functions for offline analysis\n",
    "def analyze_historical_data(parquet_path=\"/tmp/trump_sentiment_data\"):\n",
    "    \"\"\"Analyze historical sentiment data\"\"\"\n",
    "    \n",
    "    print(\"Loading historical sentiment data...\")\n",
    "    historical_df = spark.read.parquet(parquet_path)\n",
    "    \n",
    "    print(\"Historical Analysis Results:\")\n",
    "    print(f\"Total comments analyzed: {historical_df.count()}\")\n",
    "    \n",
    "    # Overall sentiment distribution\n",
    "    historical_df.groupBy(\"sentiment\").count().show()\n",
    "    \n",
    "    # Top subreddits by activity\n",
    "    historical_df.groupBy(\"subreddit\") \\\n",
    "                 .agg(count(\"*\").alias(\"comment_count\"),\n",
    "                      avg(\"polarity\").alias(\"avg_sentiment\")) \\\n",
    "                 .orderBy(desc(\"comment_count\")) \\\n",
    "                 .show(20)\n",
    "    \n",
    "    # Sentiment trends over time\n",
    "    historical_df.groupBy(to_date(\"created_date\").alias(\"date\"), \"sentiment\") \\\n",
    "                 .count() \\\n",
    "                 .orderBy(\"date\") \\\n",
    "                 .show()\n",
    "    \n",
    "    return historical_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
